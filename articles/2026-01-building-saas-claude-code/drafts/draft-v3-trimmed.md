# Building a Production SaaS with Claude Code: 468 Commits, 695 Tests, Real Lessons

Four weeks ago, there was an idea: build an AI advisory board where multiple AI experts could debate each other. Today, it's deployed on Vercel as a pilot with friends and family - 695 automated tests, zero production incidents, and early users getting simultaneous advice from 3-5 AI advisors.

This isn't another "I built an MVP in a weekend" story. It's 468 commits, 114 TypeScript files, 24 REST API endpoints, and 4 weeks of focused development as a solo founder using Claude Code as my development partner. Most importantly, it's honest: the wins, the late-night debugging sessions, and the lessons that only come from building something real.

Here's what I learned building Boailoop (Board of Advisors) with Claude Code - including what worked, what didn't, and the actual time it took.

**TL;DR:**

- Built production SaaS MVP in 4 weeks (468 commits, 695 tests, 24 API endpoints)
- Claude Code handled ~54% of work (implementation, tests, docs)
- Key lesson: Partnership > automation (AI suggests, human decides)
- 2-3x productivity gain, but still required 306 hours of focused work
- Now deployed as pilot on Vercel with real users

## The Idea: Why This Isn't a Weekend Hack

Boailoop is a multi-AI advisory board platform that lets founders, PMs, and consultants interact with 3-5 specialized AI advisor personas simultaneously. Unlike ChatGPT or Claude where you get one perspective, Boailoop gives you multiple viewpoints - and the advisors can even debate each other.

**Core features that make it complex:**

- Multi-persona orchestration (sequential LLM calls with state management)
- Server-Sent Events (SSE) streaming for real-time responses
- Row Level Security (RLS) on all 7 database tables
- Persistent advisory sessions ("loops") with full history
- Document integration (PDF upload and context injection)
- Debate mode where advisors challenge each other
- AI-powered summaries and decision tracking

**Tech stack:** Next.js 14, TypeScript, Tailwind + shadcn/ui, Supabase (PostgreSQL + Auth), Anthropic Claude API, Vitest, Playwright, GitHub Actions, Vercel, and Axiom for production monitoring.

This wasn't going to be a weekend project. But with Claude Code handling the repetitive heavy lifting, could a solo founder build it in a month? Let's find out.

---

![Multi-advisor discussion interface showing Maya Chen (CTO), Victoria Adeleke (Marketing), and Sam Rodriguez (CFO) debating a business question](Screenshot%202026-01-02%20at%2011.12.07.png)

_Multi-advisor discussion interface showing Maya Chen (CTO), Victoria Adeleke (Marketing), and Sam Rodriguez (CFO) debating a business question. Note the AI Summary at bottom, token usage tracking (21,656 tokens), and loop history sidebar._

---

### How Multi-Advisor Orchestration Works

The core technical challenge is coordinating multiple AI personas to respond to the same question while maintaining distinct perspectives. Here's the simplified architecture:

---

![Multi-Advisor Loop Architecture diagram](architecture-diagram.png)

_Multi-Advisor Loop Architecture: User questions trigger sequential Claude API calls (one per persona), with responses streaming back via Server-Sent Events (SSE) for real-time updates._

---

**The orchestration flow:**

1. User asks a business question
2. Backend loads context (persona profiles + conversation history from Supabase)
3. Loop Orchestrator makes **sequential** calls to Claude API (one per persona)
4. Each persona gets a unique system prompt (CTO, Marketing, CFO perspectives)
5. Responses stream back via Server-Sent Events (SSE) for real-time updates
6. UI displays 3-5 perspectives simultaneously
7. All messages saved to PostgreSQL for persistent loop history

**Key design choice:** Sequential (not parallel) LLM calls to manage costs and ensure stable streaming.

---

## Sprint 1: Foundation (1 week) - Testing from Day One

The first week set the tone for everything that followed. Instead of rushing to build features, I made a decision that would prove invaluable: comprehensive testing from the start.

**What I built:**

- Database schema (7 tables with RLS policies and strategic indexes)
- Persona CRUD API (5 RESTful endpoints)
- Board configuration system
- Persona library UI
- **132 automated tests** (59 unit + 62 integration + 11 E2E)

**The metrics:** 29 commits, +17,609 lines of code, 100% API endpoint test coverage.

Claude Code excelled at the foundation work. I described the database schema I needed, and it generated proper PostgreSQL tables with relationship constraints that actually made sense:

```sql
-- Generated by Claude Code (with my refinements)
CREATE TABLE loops (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  selected_persona_ids UUID[] NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  CONSTRAINT check_persona_count CHECK (
    array_length(selected_persona_ids, 1) BETWEEN 3 AND 5
  )
);
```

That constraint? I told Claude Code "users must select 3-5 personas per board" and it translated that business rule into enforceable database logic. This pattern repeated throughout Sprint 1: I provided the requirements, Claude Code generated solid implementations.

**The first challenge:** Getting tests to authenticate with local Supabase. Claude Code suggested generating JWT tokens manually for test users - a pattern I wouldn't have thought of. Within an hour, I had isolated test users that could run in parallel without conflicts.

> **Key learning:** Those 132 tests from day one prevented countless regressions as the codebase evolved. Every time I refactored something in Sprint 3 or 4, those tests caught breaking changes before they hit production.

## Sprint 2: Multi-Advisor Loops (1 week) - When Claude Code Caught What I Missed

Sprint 2 was about building the core differentiator: multi-advisor orchestration with real-time streaming responses.

**What I built:**

- LoopService with async generator pattern for streaming
- SSE (Server-Sent Events) API endpoints
- Loop history with search and filtering
- CI/CD pipeline with GitHub Actions
- Comprehensive error handling

**The metrics:** 81 commits, +1,537 lines, 191 total tests (+45% growth).

Claude Code's async generator pattern for streaming was elegant:

```typescript
// Generated by Claude Code (conceptual example)
async function* streamPersonaResponses(personas, userMessage) {
  for (const persona of personas) {
    const stream = await claude.messages.stream({
      model: "claude-sonnet-4-5",
      messages: [{ role: "user", content: userMessage }],
      system: persona.system_prompt,
    });

    for await (const chunk of stream) {
      yield { personaId: persona.id, content: chunk.text };
    }
  }
}
```

But the most valuable moment came mid-sprint. I was implementing loop search functionality when Claude Code flagged something in its own generated code:

> "This ILIKE query is vulnerable to SQL injection through wildcard characters."

I had completely missed it. Claude Code not only detected the security issue but suggested the fix: an `escapeILIKE()` utility function that sanitizes `%`, `_`, and `\` characters. We caught and fixed a production security bug before it ever shipped.

**The mid-sprint pivot:** I noticed test coverage was only 23% - way too low. Mid-sprint, I made a hard call: defer the feedback system, focus on quality. We added 59 new unit tests in one week, bringing coverage to 50%. Slower feature velocity, but the foundation was now solid.

**Key learning:** Quality pivots are sometimes more valuable than feature velocity. Those extra tests enabled the 200% velocity we'd hit in Sprint 3.

## Sprint 3: Polish & Dashboard (1 week) - 200% Velocity Without Breaking Production

Sprint 3 was where the partnership with Claude Code really clicked. 26 issues completed. 129 commits. 28 PRs merged. Zero production incidents.

**What I built:**

- Dashboard overview widget
- Per-loop board configuration (custom advisor selection per session)
- Feedback system (upvote/downvote on messages)
- Loop export (PDF + Markdown)
- RAG preview (markdown document upload)
- Complete E2E test refactoring

**The metrics:** 441 total tests (+131% from Sprint 2), 341 unit tests (+189% growth).

The standout achievement was E2E test refactoring. I had 14 failing E2E tests with brittle CSS selectors. Claude Code suggested replacing them with semantic `data-testid` attributes and helper functions. The result? Code duplication reduced by 60%, test reliability improved to 100%, and the tests were re-enabled in CI.

**Key learning:** When Claude Code suggested the `data-testid` pattern and generated helper functions, it wasn't just fixing tests - it was teaching me a better testing approach that I'll use in future projects.

## Sprint 4: MVP Feature Complete (1 week) - The Customer Pivot

Sprint 4 started with a plan: implement parallel LLM execution, add API versioning, and build advanced document management. Then a conversation with the customer changed everything.

**We deprioritized three planned features and prioritized three different ones:**

- ~~Advanced document management~~ → Axiom logging (production monitoring)
- ~~Parallel LLM execution~~ → Streaming UX improvements (layout stability)
- ~~API versioning~~ → 5-turn rate limiting (cost control)

This pivot delivered a better MVP. Production monitoring meant we could debug real issues. Streaming UX improvements made the product feel polished. Rate limiting gave us cost predictability.

**What I built:**

- Axiom logging infrastructure with HOF wrapper pattern
- Pre-stream skeleton placeholders (no more layout jumps)
- 5-turn loop limit with auto-close and decision capture
- E2E tests in CI with MSW mocking
- Debate mode, decision tracking, AI summaries
- PDF upload and processing

**The metrics:** 143 commits, +33,847 net lines, 695 total tests (673 passing, 96.8% pass rate), 264% test growth from Sprint 2.

---

![AI-powered decision extraction modal showing extracted decisions with confidence levels](Screenshot%202026-01-02%20at%2011.14.15.png)

_AI-powered decision extraction modal showing three decisions with confidence levels (75%, 65%, 55%) and which advisors influenced each decision. Users can edit decisions before saving for accountability tracking._

---

**The PDF processing saga:** This is where things got real. I needed PDF text extraction that worked in Next.js/Vercel's serverless environment.

- **Attempt 1:** `pdf-parse` - Failed (requires Node.js native modules)
- **Attempt 2:** `pdfjs-dist` - Failed (canvas dependency issues in serverless)
- **Attempt 3:** `pdf2json` - Success (pure JavaScript, serverless-compatible)

Three libraries, multiple late-night debugging sessions, ~8 hours total. Claude Code helped compare the libraries and debug each failure, but ultimately this was human persistence solving a compatibility puzzle.

> **Key learning:** Research library compatibility BEFORE deep integration. Claude Code can suggest options, but environmental constraints require human debugging and iteration.

## What Claude Code Did Exceptionally Well

After 468 commits together, here's where Claude Code consistently excelled:

**1. Database schema generation** - Saved literal days of work. Proper relationships, RLS policies, strategic indexes, all generated from requirements.

**2. API endpoint scaffolding** - Consistent RESTful patterns, Zod validation schemas, error handling. Every API endpoint followed the same clean pattern because Claude Code templated it.

**3. Test suite creation** - 695 tests across three tiers. Claude Code wrote comprehensive test cases I wouldn't have thought of, especially edge cases in integration tests.

**4. Bug detection** - That SQL injection vulnerability? Claude Code caught it. N+1 query patterns? Claude Code spotted them. Performance bottlenecks? Claude Code identified them.

**5. SSE streaming implementation** - The async generator pattern for streaming was something I'd never implemented before. Claude Code not only generated it but explained why it worked.

**Impact:** Claude Code handled approximately 54% of the development time - implementation, tests, and documentation. This freed me to focus on architecture, product decisions, and the complex debugging that required human judgment.

## Where I Had to Take Control

Claude Code is powerful, but it's not autopilot. Here's where human decision-making was critical:

**Architecture decisions:** Should multi-persona orchestration be sequential or parallel? How should the database schema handle persona relationships? What testing strategy makes sense? These required understanding the product vision and making trade-offs.

**Complex debugging:** The PDF processing saga. The E2E test Playwright conflicts. Integration test flakiness. These required systematic investigation, trial and error, and understanding environmental constraints that Claude Code couldn't fully model.

**Product priorities:** Which features to build first? When to pivot from velocity to quality? When to defer features based on customer collaboration? These are business decisions that require understanding user needs and market context.

**Performance optimization:** Database query optimization (eliminating N+1 patterns), API call deduplication (SWR cache strategy), CI/CD pipeline speedup (matrix parallelization). Claude Code identified issues, but the optimization strategy required understanding the system holistically.

**Security review:** RLS policy correctness, environment variable management, secrets handling in CI/CD. While Claude Code caught the SQL injection bug, comprehensive security review required human paranoia.

> **The partnership pattern:** Claude Code suggested approaches, I made decisions. Claude Code generated code, I reviewed and refined. Claude Code wrote tests, I ensured coverage. Iterative partnership, not full automation.

## The Real Costs (The Part Other Articles Skip)

Let's talk honestly about time investment, because "4 weeks" doesn't tell the full story.

**Total estimated hours:** ~306 hours over 4 weeks (~11 hours/day average)

| Sprint   | Hours          | Focus Area                              |
| -------- | -------------- | --------------------------------------- |
| Sprint 1 | ~64h (9h/day)  | Database setup and testing foundation   |
| Sprint 2 | ~78h (11h/day) | Multi-advisor orchestration             |
| Sprint 3 | ~72h (10h/day) | Dashboard and E2E refactoring           |
| Sprint 4 | ~92h (12h/day) | Production readiness and PDF processing |

**Late-night work:** Multiple debugging sessions until 2-3 AM. The PDF processing compatibility issues. Integration test stability. E2E test re-enablement.

**What this is NOT:** This wasn't a casual weekend hack. It's 4 weeks of focused, intensive development with proper testing, security, and production infrastructure.

**The productivity multiplier:** Estimated 2-3x faster than solo development without AI assistance. Claude Code contributed ~166 hours (54%), human contribution ~140 hours (46%).

## Key Learnings: What Worked and What Didn't

**What worked:**

1. **Testing from day one** - Those 132 tests in Sprint 1 prevented regressions and enabled aggressive refactoring later.

2. **Quality pivots** - Mid-sprint adjustments based on test coverage gaps improved long-term velocity.

3. **Customer collaboration** - Deprioritizing 3 features to prioritize 3 better features delivered a stronger MVP.

4. **Pre-commit hooks** - ESLint, Prettier, and TypeScript checks caught issues before they reached CI.

5. **Documentation concurrency** - Writing docs during development, not after, preserved knowledge and prevented information loss.

6. **Claude Code for repetition** - API scaffolding, test generation, documentation drafting.

7. **Human for decisions** - Architecture, priorities, complex debugging, security review.

**What didn't work:**

1. **PDF library iteration** - Three library attempts wasted ~8 hours. Should have researched serverless compatibility first.

2. **E2E test approach** - Should have used MSW mocking from the start instead of dealing with Playwright conflicts later.

3. **Late-night debugging** - Effective but unsustainable. Better time management would have prevented some of the crunch.

**Top 5 insights for other builders:**

1. **Partnership beats automation** - Claude Code + human decisions > either alone. Let AI handle repetitive tasks, but own your architecture.

2. **Test early, test often** - 695 tests enabled aggressive refactoring without fear. Testing isn't overhead, it's freedom.

3. **Research compatibility first** - Especially for serverless environments. Test integration before deep commitment.

4. **Customer pivots add value** - Monitoring and UX polish proved more valuable than parallel LLM execution for our MVP.

5. **Documentation preserves knowledge** - Write it concurrently with code, not after. Future you will thank present you.

## The Result: MVP Feature Complete in 4 Weeks

**Deployed on Vercel** as a pilot for friends and family (managing Claude API costs while gathering feedback).

**Final metrics:**

- 468 commits, 114 TypeScript files
- 24 REST API endpoints, 7 database tables
- 695 tests (96.8% pass rate)
- Zero production incidents
- Comprehensive monitoring (Axiom)
- Full CI/CD pipeline (GitHub Actions → Vercel)

**Core features delivered:**

- Multi-advisor perspectives (3-5 experts per question)
- Debate mode (advisors discuss and challenge each other)
- Decision tracking (accountability and outcomes)
- AI-powered summaries (actionable insights)
- Document integration (PDF upload with context)
- Persistent advisory sessions (loop history with search)

**Production ready:** Not a prototype. Not a proof of concept. A production SaaS with comprehensive testing, monitoring, security, and infrastructure.

## Conclusion: Partnership, Not Autopilot

Building Boailoop with Claude Code delivered a production-ready SaaS MVP in 4 weeks that would have taken 2-3x longer solo - not because AI wrote all the code, but because it freed me to focus on what mattered: architecture, product decisions, and quality.

**The partnership breakdown:**

- Claude Code: 54% of time (implementation, tests, docs)
- Human: 46% of time (decisions, architecture, debugging)
- Result: 2-3x productivity gain with production-quality output

**The key insight:** Claude Code isn't replacing developers. It's amplifying what we can do by handling the repetitive work - API scaffolding, test generation, documentation drafting - so we can focus on the creative, strategic, and complex work that requires human judgment.

**For other builders:** If you're exploring AI-assisted development, try Claude Code - but remember it's a partnership, not autopilot. Own your architecture decisions, test relentlessly, document concurrently, and don't skip the hard debugging. That's where the real learning happens.

And when you hit that moment at 2 AM debugging a serverless PDF processing issue? That's not a failure of AI assistance. That's the messy reality of building something real. Embrace it.

---

**Interested in trying Boailoop?** It's currently in pilot mode with friends and family as I manage Claude API costs. If you'd like early access to test it out and provide feedback, drop a comment below and I'll reach out!

**Building with AI assistance?** I'd love to hear your experiences - what worked, what didn't, and what surprised you. Share your story in the comments.

**P.S.** This article outline was created with Claude Code - practicing what I preach about human-AI partnership. I made the structural decisions, Claude Code helped organize the insights, and together we found the narrative that felt authentic.

---

_#ClaudeCode #BuildInPublic #AIAssistedDevelopment #ProductDevelopment #SaaS_
